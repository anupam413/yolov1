{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3310,"status":"ok","timestamp":1687517153288,"user":{"displayName":"Anupam Wagle","userId":"02083722924920495805"},"user_tz":-345},"id":"KWuCWBwc01dr","outputId":"c282bab2-638d-4d3f-96d0-cd5932f4e32d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VzJRHcL4zLAj"},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"shamutKmzNc_"},"outputs":[],"source":["!unzip \"/content/drive/MyDrive/archive.zip\" -d \"./data/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnkSRSu9zLAk"},"outputs":[],"source":["\n","# (kernel_size, filters, stride, padding)\n","architecture_config = [\n","    (7, 64, 2, 3),\n","    \"M\",\n","    (3, 192, 1, 1),\n","    \"M\",\n","    (1, 128, 1, 0),\n","    (3, 256, 1, 1),\n","    (1, 256, 1, 0),\n","    (3, 512, 1, 1),\n","    \"M\",\n","    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n","    (1, 512, 1, 0),\n","    (3, 1024, 1, 1),\n","    \"M\",\n","    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n","    (3, 1024, 1, 1),\n","    (3, 1024, 2, 1),\n","    (3, 1024, 1, 1),\n","    (3, 1024, 1, 1),\n","]\n","\n","\n","class CNNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, **kwargs):\n","        super(CNNBlock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n","        self.batchnorm = nn.BatchNorm2d(out_channels)\n","        self.leakyrelu = nn.LeakyReLU(0.1)\n","\n","    def forward(self, x):\n","        return self.leakyrelu(self.batchnorm(self.conv(x)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DEjtMfG4zLAk"},"outputs":[],"source":["class Yolov1(nn.Module):\n","    def __init__(self, in_channels=3, **kwargs):\n","        super(Yolov1, self).__init__()\n","        self.architecture = architecture_config\n","        self.in_channels = in_channels\n","        self.darknet = self._create_conv_layers(self.architecture)\n","        self.fcs = self._create_fcs(**kwargs)\n","\n","    def forward(self, x):\n","        x = self.darknet(x)\n","        return self.fcs(torch.flatten(x, start_dim=1))\n","\n","    def _create_conv_layers(self, architecture):\n","        layers = []\n","        in_channels = self.in_channels\n","\n","        for x in architecture:\n","            if type(x) == tuple:\n","                layers += [\n","                    CNNBlock(\n","                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3],\n","                    )\n","                ]\n","                in_channels = x[1]\n","\n","            elif type(x) == str:\n","                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n","\n","            elif type(x) == list:\n","                conv1 = x[0]\n","                conv2 = x[1]\n","                num_repeats = x[2]\n","\n","                for _ in range(num_repeats):\n","                    layers += [\n","                        CNNBlock(\n","                            in_channels,\n","                            conv1[1],\n","                            kernel_size=conv1[0],\n","                            stride=conv1[2],\n","                            padding=conv1[3],\n","                        )\n","                    ]\n","                    layers += [\n","                        CNNBlock(\n","                            conv1[1],\n","                            conv2[1],\n","                            kernel_size=conv2[0],\n","                            stride=conv2[2],\n","                            padding=conv2[3],\n","                        )\n","                    ]\n","                    in_channels = conv2[1]\n","\n","        return nn.Sequential(*layers)\n","\n","    def _create_fcs(self, split_size, num_boxes, num_classes):\n","        S, B, C = split_size, num_boxes, num_classes\n","\n","        # In original paper this should be\n","        # nn.Linear(1024*S*S, 4096),\n","        # nn.LeakyReLU(0.1),\n","        # nn.Linear(4096, S*S*(B*5+C))\n","\n","        return nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(1024 * S * S, 4096),\n","            nn.Dropout(0.5),\n","            nn.LeakyReLU(0.1),\n","            nn.Linear(4096, S * S * (C + B * 5)),\n","        )\n"]},{"cell_type":"code","source":["# Testing the model\n","model = Yolov1(split_size=7, num_boxes=2, num_classes=20)\n","x = torch.randn((2,3,448,448))\n","print(model(x).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_mgLZxXuwoa","executionInfo":{"status":"ok","timestamp":1687517156792,"user_tz":-345,"elapsed":3514,"user":{"displayName":"Anupam Wagle","userId":"02083722924920495805"}},"outputId":"cde778bf-1c20-42c0-9bf1-fc0d5077caac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 1470])\n"]}]},{"cell_type":"code","source":["# Generate the summary of the model\n","from torchsummary import summary\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","summary(model.to(device), (3,448,448))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qUNlOjLFu08p","executionInfo":{"status":"ok","timestamp":1687517157565,"user_tz":-345,"elapsed":776,"user":{"displayName":"Anupam Wagle","userId":"02083722924920495805"}},"outputId":"6de3c55a-89a2-4f9a-b3a8-16a01efaf475"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 224, 224]           9,408\n","       BatchNorm2d-2         [-1, 64, 224, 224]             128\n","         LeakyReLU-3         [-1, 64, 224, 224]               0\n","          CNNBlock-4         [-1, 64, 224, 224]               0\n","         MaxPool2d-5         [-1, 64, 112, 112]               0\n","            Conv2d-6        [-1, 192, 112, 112]         110,592\n","       BatchNorm2d-7        [-1, 192, 112, 112]             384\n","         LeakyReLU-8        [-1, 192, 112, 112]               0\n","          CNNBlock-9        [-1, 192, 112, 112]               0\n","        MaxPool2d-10          [-1, 192, 56, 56]               0\n","           Conv2d-11          [-1, 128, 56, 56]          24,576\n","      BatchNorm2d-12          [-1, 128, 56, 56]             256\n","        LeakyReLU-13          [-1, 128, 56, 56]               0\n","         CNNBlock-14          [-1, 128, 56, 56]               0\n","           Conv2d-15          [-1, 256, 56, 56]         294,912\n","      BatchNorm2d-16          [-1, 256, 56, 56]             512\n","        LeakyReLU-17          [-1, 256, 56, 56]               0\n","         CNNBlock-18          [-1, 256, 56, 56]               0\n","           Conv2d-19          [-1, 256, 56, 56]          65,536\n","      BatchNorm2d-20          [-1, 256, 56, 56]             512\n","        LeakyReLU-21          [-1, 256, 56, 56]               0\n","         CNNBlock-22          [-1, 256, 56, 56]               0\n","           Conv2d-23          [-1, 512, 56, 56]       1,179,648\n","      BatchNorm2d-24          [-1, 512, 56, 56]           1,024\n","        LeakyReLU-25          [-1, 512, 56, 56]               0\n","         CNNBlock-26          [-1, 512, 56, 56]               0\n","        MaxPool2d-27          [-1, 512, 28, 28]               0\n","           Conv2d-28          [-1, 256, 28, 28]         131,072\n","      BatchNorm2d-29          [-1, 256, 28, 28]             512\n","        LeakyReLU-30          [-1, 256, 28, 28]               0\n","         CNNBlock-31          [-1, 256, 28, 28]               0\n","           Conv2d-32          [-1, 512, 28, 28]       1,179,648\n","      BatchNorm2d-33          [-1, 512, 28, 28]           1,024\n","        LeakyReLU-34          [-1, 512, 28, 28]               0\n","         CNNBlock-35          [-1, 512, 28, 28]               0\n","           Conv2d-36          [-1, 256, 28, 28]         131,072\n","      BatchNorm2d-37          [-1, 256, 28, 28]             512\n","        LeakyReLU-38          [-1, 256, 28, 28]               0\n","         CNNBlock-39          [-1, 256, 28, 28]               0\n","           Conv2d-40          [-1, 512, 28, 28]       1,179,648\n","      BatchNorm2d-41          [-1, 512, 28, 28]           1,024\n","        LeakyReLU-42          [-1, 512, 28, 28]               0\n","         CNNBlock-43          [-1, 512, 28, 28]               0\n","           Conv2d-44          [-1, 256, 28, 28]         131,072\n","      BatchNorm2d-45          [-1, 256, 28, 28]             512\n","        LeakyReLU-46          [-1, 256, 28, 28]               0\n","         CNNBlock-47          [-1, 256, 28, 28]               0\n","           Conv2d-48          [-1, 512, 28, 28]       1,179,648\n","      BatchNorm2d-49          [-1, 512, 28, 28]           1,024\n","        LeakyReLU-50          [-1, 512, 28, 28]               0\n","         CNNBlock-51          [-1, 512, 28, 28]               0\n","           Conv2d-52          [-1, 256, 28, 28]         131,072\n","      BatchNorm2d-53          [-1, 256, 28, 28]             512\n","        LeakyReLU-54          [-1, 256, 28, 28]               0\n","         CNNBlock-55          [-1, 256, 28, 28]               0\n","           Conv2d-56          [-1, 512, 28, 28]       1,179,648\n","      BatchNorm2d-57          [-1, 512, 28, 28]           1,024\n","        LeakyReLU-58          [-1, 512, 28, 28]               0\n","         CNNBlock-59          [-1, 512, 28, 28]               0\n","           Conv2d-60          [-1, 512, 28, 28]         262,144\n","      BatchNorm2d-61          [-1, 512, 28, 28]           1,024\n","        LeakyReLU-62          [-1, 512, 28, 28]               0\n","         CNNBlock-63          [-1, 512, 28, 28]               0\n","           Conv2d-64         [-1, 1024, 28, 28]       4,718,592\n","      BatchNorm2d-65         [-1, 1024, 28, 28]           2,048\n","        LeakyReLU-66         [-1, 1024, 28, 28]               0\n","         CNNBlock-67         [-1, 1024, 28, 28]               0\n","        MaxPool2d-68         [-1, 1024, 14, 14]               0\n","           Conv2d-69          [-1, 512, 14, 14]         524,288\n","      BatchNorm2d-70          [-1, 512, 14, 14]           1,024\n","        LeakyReLU-71          [-1, 512, 14, 14]               0\n","         CNNBlock-72          [-1, 512, 14, 14]               0\n","           Conv2d-73         [-1, 1024, 14, 14]       4,718,592\n","      BatchNorm2d-74         [-1, 1024, 14, 14]           2,048\n","        LeakyReLU-75         [-1, 1024, 14, 14]               0\n","         CNNBlock-76         [-1, 1024, 14, 14]               0\n","           Conv2d-77          [-1, 512, 14, 14]         524,288\n","      BatchNorm2d-78          [-1, 512, 14, 14]           1,024\n","        LeakyReLU-79          [-1, 512, 14, 14]               0\n","         CNNBlock-80          [-1, 512, 14, 14]               0\n","           Conv2d-81         [-1, 1024, 14, 14]       4,718,592\n","      BatchNorm2d-82         [-1, 1024, 14, 14]           2,048\n","        LeakyReLU-83         [-1, 1024, 14, 14]               0\n","         CNNBlock-84         [-1, 1024, 14, 14]               0\n","           Conv2d-85         [-1, 1024, 14, 14]       9,437,184\n","      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n","        LeakyReLU-87         [-1, 1024, 14, 14]               0\n","         CNNBlock-88         [-1, 1024, 14, 14]               0\n","           Conv2d-89           [-1, 1024, 7, 7]       9,437,184\n","      BatchNorm2d-90           [-1, 1024, 7, 7]           2,048\n","        LeakyReLU-91           [-1, 1024, 7, 7]               0\n","         CNNBlock-92           [-1, 1024, 7, 7]               0\n","           Conv2d-93           [-1, 1024, 7, 7]       9,437,184\n","      BatchNorm2d-94           [-1, 1024, 7, 7]           2,048\n","        LeakyReLU-95           [-1, 1024, 7, 7]               0\n","         CNNBlock-96           [-1, 1024, 7, 7]               0\n","           Conv2d-97           [-1, 1024, 7, 7]       9,437,184\n","      BatchNorm2d-98           [-1, 1024, 7, 7]           2,048\n","        LeakyReLU-99           [-1, 1024, 7, 7]               0\n","        CNNBlock-100           [-1, 1024, 7, 7]               0\n","         Flatten-101                [-1, 50176]               0\n","          Linear-102                 [-1, 4096]     205,524,992\n","         Dropout-103                 [-1, 4096]               0\n","       LeakyReLU-104                 [-1, 4096]               0\n","          Linear-105                 [-1, 1470]       6,022,590\n","================================================================\n","Total params: 271,716,734\n","Trainable params: 271,716,734\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 2.30\n","Forward/backward pass size (MB): 436.89\n","Params size (MB): 1036.52\n","Estimated Total Size (MB): 1475.71\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["### LOSS FUNCTION\n"],"metadata":{"id":"KKkIXE06xzef"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class YoloLoss(nn.Module):\n","  def __init__(self, S=7, B=2, C=20):\n","    super(YoloLoss, self).__init__()\n","    self.mse = nn.MSELoss(reduction=\"sum\") # reduction=\"sum\" indicates that the losses will be summed up\n","    self.S = S\n","    self.B = B\n","    self.C = C\n","\n","    self.lambda_noobj = 0.5\n","    self.lambda_coord = 5\n","\n","  def forward(self, predictions, target):\n","    predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B*5)\n","\n","    iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n","    iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n","\n","    ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n","    iou_maxes, bestbox = torch.max(ious, dim=0) # Store the maximum values and corresponding indices, respectively\n","\n","    # Get the confidence probability and unsqueeze in 3rd dim\n","    exists_box = target[..., 20].unsqueeze(3) # Tells us if there is an object in cell (i)\n","\n","    #=======================#\n","    # FOR BOUNDING BOX LOSS #\n","    #=======================#\n","    box_predictions = exists_box * (\n","        (\n","          bestbox * predictions[..., 26:30] # if second bbox is the best\n","          + (1 - bestbox) * predictions[..., 21:25] # if first bbox is the best\n","        )\n","    )\n","\n","    box_targets = exists_box * target[..., 21:25]\n","\n","    # Taking square root of width and height as required\n","    box_predictions[...,2:4] = torch.sign(box_predictions[...,2:4]) * torch.sqrt(\n","        torch.abs(box_predictions[...,2:4] + 1e-6))\n","    box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n","\n","    # Calculate the loss\n","    # end_dim=-2 indicates that all dimensions except the last two should be flattened because (N,S,S,4) --> (N*S*S, 4)\n","    box_loss = self.mse(\n","            torch.flatten(box_predictions, end_dim=-2),\n","            torch.flatten(box_targets, end_dim=-2))\n","\n","    #=================#\n","    # FOR OBJECT LOSS #\n","    #=================#\n","    pred_box = (\n","        bestbox*predictions[..., 25:26] # if second box is best, take its confidence value\n","        + (1-bestbox)*predictions[..., 20:21] # if first box is best, take its confidence value\n","    )\n","\n","    object_loss = self.mse(\n","        torch.flatten(exists_box * pred_box),\n","        torch.flatten(exists_box * target[...,20:21])\n","    )\n","\n","    #====================#\n","    # FOR NO OBJECT LOSS #\n","    #====================#\n","\n","    # For first bbox\n","    no_object_loss = self.mse(\n","        torch.flatten((1 - exists_box) * predictions[...,20:21], start_dim=1), # (N,S,S,1) --> (N, S*S*1)\n","        torch.flatten((1 - exists_box) * target[...,20:21], start_dim=1)\n","    )\n","\n","    # For second bbox\n","    no_object_loss += self.mse(\n","        torch.flatten((1 - exists_box) * predictions[...,25:26], start_dim=1),\n","        torch.flatten((1 - exists_box) * target[...,20:21], start_dim=1)\n","    )\n","\n","    #================#\n","    # FOR CLASS LOSS #\n","    #================#\n","    class_loss = self.mse(\n","        torch.flatten(exists_box * predictions[..., :20], end_dim=-2), # (N,S,S,20)-->(N*S*S, 20)\n","        torch.flatten(exists_box * target[..., :20], end_dim=-2)\n","    )\n","\n","    # ACTUAL LOSS\n","    loss = (\n","        self.lambda_coord * box_loss\n","        + object_loss\n","        + self.lambda_noobj * no_object_loss\n","        + class_loss\n","    )\n","\n","    return loss\n"],"metadata":{"id":"uxMGgDA_x11y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### utility function"],"metadata":{"id":"HpEB53Yeu12p"}},{"cell_type":"code","source":["img_labels = {0:'aeroplane',\n","              1:'bicycle',\n","              2:'bird',\n","              3:'boat',\n","              4:'bottle',\n","              5:'bus',\n","              6:'car',\n","              7:'cat',\n","              8:'chair',\n","              9:'cow',\n","              10:'diningtable',\n","              11:'dog',\n","              12:'horse',\n","              13:'motorbike',\n","              14:'person',\n","              15:'pottedplant',\n","              16:'sheep',\n","              17:'sofa',\n","              18:'train',\n","              19:'tvmonitor'}"],"metadata":{"id":"xC0UetrqxyQ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n","  \"\"\"\n","  Calculates intersection over union\n","\n","  Parameters:\n","    boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n","    boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n","    box_format (str): midpoint/ corners, if boxes (x,y,w,h) or (x1, y1, x2, y2)\n","\n","  Returns:\n","    tensor: Intersection over union for all examples\n","\n","  Note:\n","    The `...` is used for indexing all elements in the preceding dimensions\n","    and `0:1` represents the range of indices to extract along the last dimension\n","  \"\"\"\n","\n","  if box_format == 'midpoint':\n","    box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n","    box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n","    box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n","    box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n","\n","    box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n","    box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n","    box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n","    box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n","\n","  elif box_format == 'corners':\n","    box1_x1 = boxes_preds[..., 0:1]\n","    box1_y1 = boxes_preds[..., 1:2]\n","    box1_x2 = boxes_preds[..., 2:3]\n","    box1_y2 = boxes_preds[..., 3:4] # Slicing this way to maintain the shape i.e (N,1) where, N is the number of bboxes\n","\n","    box2_x1 = boxes_labels[..., 0:1]\n","    box2_y1 = boxes_labels[..., 1:2]\n","    box2_x2 = boxes_labels[..., 2:3]\n","    box2_y2 = boxes_labels[..., 3:4]\n","\n","  x1 = torch.max(box1_x1, box2_x1)\n","  y1 = torch.max(box1_y1, box2_y1)\n","  x2 = torch.max(box1_x2, box2_x2)\n","  y2 = torch.max(box1_y2, box2_y2)\n","\n","  # .clamp(0) is for the case when they do not intersect\n","  intersection = (x2-x1).clamp(0) * (y2-y1).clamp(0) # length * breadth\n","\n","  box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1)) # Absolute so that area is not negative\n","  box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1)) # Absolute so that area is not negative\n","\n","  # IOU = Area of intersection / Area of Union\n","  return intersection / (box1_area + box2_area - intersection + 1e-6)\n"],"metadata":{"id":"1H3SBZC6z2hB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def non_max_supression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n","  \"\"\"\n","  Note: bboxes input should be list of bounding boxes\n","  i.e bboxes = [[1, 0.9, x1, y1, x2, y2], ..] # Each bounding box --> [class, probability, x1, y1, x2, y2]\n","\n","  \"\"\"\n","\n","  # Validate the input\n","  assert type(bboxes) == list\n","\n","  # Discard all the bounding box < probability threshold\n","  bboxes = [box for box in bboxes if box[1] > threshold]\n","\n","  # Sort the bboxes in descending order based on their probabilities\n","  bboxes = sorted(bboxes, key=lambda x:x[1], reverse=True)\n","\n","  # Create empty list for bboxes to append after NMS\n","  bboxes_after_nms = []\n","\n","  while bboxes:\n","    chosen_box = bboxes.pop(0) # Select and remove the bounding box with largest probability from bboxes list\n","\n","    # New list comprehension for different class or same class having IoU less than threshold\n","    bboxes = [\n","        box\n","        for box in bboxes\n","        if box[0] != chosen_box[0] # Checks if the class label of box is different from the class label of chosen_box\n","        or intersection_over_union(\n","            torch.tensor(chosen_box[2:]),\n","            torch.tensor(box[2:]),\n","            box_format = box_format,\n","        ) < iou_threshold # Checks if IoU < iou_threshold\n","    ]\n","\n","    bboxes_after_nms.append(chosen_box)\n","\n","  return bboxes_after_nms"],"metadata":{"id":"di9GFC_3z4Bp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from collections import Counter\n","\n","def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format='midpoint', num_classes=20):\n","  \"\"\"\n","  Note: pred_boxes input should be list of bounding boxes\n","  i.e pred_boxes = [[train_idx, class_pred, prob_score, x1, y1, x2, y2], ...]\n","\n","  Similarly for true_boxes\n","  \"\"\"\n","\n","  average_precisions = [] # To store average precisions of each class\n","  epsilon = 1e-6 # For numerical stability\n","\n","  for c in range(num_classes):\n","    detections = []\n","    ground_truths = []\n","\n","    for detection in pred_boxes:\n","      if detection[1] == c:\n","        detections.append(detection)\n","\n","    for true_box in true_boxes:\n","      if true_box[1] == c:\n","        ground_truths.append(true_box)\n","\n","    # Calculate the count of unique elements in the ground_truths list and stores the result in the amount_bboxes variable\n","    # For eg. img 0 has 3 bboxes, img 1 has 5 bboxes then, amount_bboxes = {0:3, 1:5}\n","    amount_bboxes = Counter([gt[0] for gt in ground_truths])\n","\n","    for key, val in amount_bboxes.items():\n","      amount_bboxes[key] = torch.zeros(val)\n","    # amount_boxes = {0: torch.tensor([0,0,0]), 1: torch.tensor([0,0,0,0,0])}\n","\n","    # Sort the bboxes in descending order based on their probabilities\n","    detections.sort(key=lambda x: x[2], reverse=True)\n","\n","    TP = torch.zeros((len(detections)))\n","    FP = torch.zeros((len(detections)))\n","    total_true_bboxes = len(ground_truths)\n","\n","    # If none exists for this class then we can safely skip\n","    if total_true_bboxes == 0:\n","      continue\n","\n","    for detection_idx, detection in enumerate(detections):\n","      # Only filter images having same index\n","      ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n","\n","      num_gts = len(ground_truth_img)\n","      best_iou = 0\n","\n","      # For selection of bbox having highest iou\n","      for idx, gt in enumerate(ground_truth_img):\n","        iou = intersection_over_union(\n","            torch.tensor(detection[3:]),\n","            torch.tensor(gt[3:]),\n","            box_format = box_format\n","        )\n","\n","        if iou > best_iou:\n","          best_iou = iou\n","          best_gt_idx = idx\n","\n","      # Categorizing either TP or FP\n","      if best_iou > iou_threshold:\n","        # Check if we haven't covered this bounding box before | '0' means we haven't covered\n","        if amount_bboxes[detection[0]][best_gt_idx] == 0:\n","          TP[detection_idx] = 1\n","          amount_bboxes[detection[0]][best_gt_idx] == 1 # Update that now it's covered\n","        else:\n","          FP[detection_idx] = 1\n","      else:\n","        FP[detection_idx] = 1\n","\n","    # [1,1,0,1,0] --> [1,2,2,3,3]\n","    TP_cumsum = torch.cumsum(TP, dim=0)\n","    FP_cumsum = torch.cumsum(FP, dim=0)\n","\n","    recalls = TP_cumsum / (total_true_bboxes + epsilon)\n","    precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n","\n","    # By adding these initial values of 1 to precisions and 0 to recalls,\n","    # the code ensures that the precision and recall values start with the appropriate initial points.\n","    precisions = torch.cat((torch.tensor([1]), precisions))\n","    recalls = torch.cat((torch.tensor([0]), recalls))\n","\n","    # Calculate the average precision by using the trapezoidal rule to compute the area under the precision-recall curve\n","    average_precisions.append(torch.trapz(precisions, recalls))\n","\n","  # Return mAP\n","  return sum(average_precisions) / len(average_precisions)\n"],"metadata":{"id":"3cNakFlNz7_3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","\n","def plot_image(image, boxes):\n","    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n","    # Convert input image to numpy array\n","    im = np.array(image)\n","    height, width, _ = im.shape\n","\n","    # Create figure and axes\n","    fig, ax = plt.subplots(1)\n","    # Display the image\n","    ax.imshow(im)\n","\n","    # box[0] is x midpoint, box[2] is width\n","    # box[1] is y midpoint, box[3] is height\n","\n","    # Create a Rectangle patch\n","    for box in boxes:\n","        cls = int(box[0])\n","        prob = box[1]\n","        box = box[2:]\n","        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n","\n","        # Calculate the top corner of bounding box\n","        upper_left_x = box[0] - box[2] / 2\n","        upper_left_y = box[1] - box[3] / 2\n","\n","        # Create Rectangle patch\n","        rect = Rectangle(\n","            (upper_left_x * width, upper_left_y * height),\n","            box[2] * width,\n","            box[3] * height,\n","            linewidth=1,\n","            edgecolor=\"r\",\n","            facecolor=\"none\",\n","        )\n","        # Add the patch to the Axes\n","        ax.add_patch(rect)\n","\n","        # Add class and probability text\n","        text = f\"{img_labels[cls]}: {prob:.2f}\"\n","        ax.text(\n","            upper_left_x * width,\n","            upper_left_y * height - 10,\n","            text,\n","            fontsize=10,\n","            color=\"r\",\n","            verticalalignment=\"top\",\n","            bbox={\"facecolor\": \"white\", \"alpha\": 0.7, \"pad\": 2},\n","        )\n","\n","    plt.show()\n","    return fig"],"metadata":{"id":"JVWTUvxm0ETh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_bboxes(\n","    loader,\n","    model,\n","    iou_threshold,\n","    threshold,\n","    pred_format=\"cells\",\n","    box_format=\"midpoint\",\n","    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n","\n","  \"\"\"\n","  Input images --> get all true boxes and predicted boxes\n","  \"\"\"\n","\n","  all_pred_boxes = []\n","  all_true_boxes = []\n","\n","  # make sure model is in eval before get bboxes\n","  model.eval()\n","  train_idx = 0 # For each batch\n","\n","  for batch_idx, (x, labels) in enumerate(loader):\n","      x = x.to(device)\n","      labels = labels.to(device)\n","\n","      with torch.no_grad():\n","          predictions = model(x)\n","\n","      batch_size = x.shape[0]\n","      true_bboxes = cellboxes_to_boxes(labels)\n","      bboxes = cellboxes_to_boxes(predictions)\n","\n","      # For every image in each batch --> NMS\n","      for idx in range(batch_size):\n","          nms_boxes = non_max_supression(\n","              bboxes[idx],\n","              iou_threshold=iou_threshold,\n","              threshold=threshold,\n","              box_format=box_format,\n","          )\n","\n","          for nms_box in nms_boxes:\n","              all_pred_boxes.append([train_idx] + nms_box)\n","\n","          for box in true_bboxes[idx]:\n","              # many will get converted to 0 pred\n","              if box[1] > threshold:\n","                  all_true_boxes.append([train_idx] + box)\n","\n","          train_idx += 1\n","\n","  model.train()\n","  return all_pred_boxes, all_true_boxes"],"metadata":{"id":"Qe7YPW5V0Fjw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_cellboxes(predictions, S=7):\n","    \"\"\"\n","    Converts bounding boxes output from Yolo with\n","    an image split size of S into entire image ratios\n","    rather than relative to cell ratios.\n","    \"\"\"\n","\n","    predictions = predictions.to(\"cpu\")\n","    batch_size = predictions.shape[0]\n","    predictions = predictions.reshape(batch_size, 7, 7, 30)\n","    bboxes1 = predictions[..., 21:25]\n","    bboxes2 = predictions[..., 26:30]\n","    scores = torch.cat(\n","        (predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0\n","    )\n","    best_box = scores.argmax(0).unsqueeze(-1)\n","    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n","    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n","    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n","    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n","    w_y = 1 / S * best_boxes[..., 2:4]\n","    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n","    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n","    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(\n","        -1\n","    )\n","    converted_preds = torch.cat(\n","        (predicted_class, best_confidence, converted_bboxes), dim=-1\n","    )\n","\n","    return converted_preds"],"metadata":{"id":"Ottuq2co0ID_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cellboxes_to_boxes(out, S=7):\n","    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n","    converted_pred[..., 0] = converted_pred[..., 0].long()\n","    all_bboxes = []\n","\n","    for ex_idx in range(out.shape[0]):\n","        bboxes = []\n","\n","        for bbox_idx in range(S * S):\n","            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n","        all_bboxes.append(bboxes)\n","\n","    return all_bboxes"],"metadata":{"id":"tB_uxLTX0JXr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WbuYi_PzLAn"},"outputs":[],"source":["import torch\n","import os\n","import pandas as pd\n","from PIL import Image\n","\n","\n","class VOCDataset(torch.utils.data.Dataset):\n","  def __init__(self, csv_file, img_dir, label_dir, S=7, B=2, C=20, transform=None):\n","    self.annotations = pd.read_csv(csv_file)\n","    self.img_dir = img_dir\n","    self.label_dir = label_dir\n","    self.transform = transform\n","    self.S = S\n","    self.B = B\n","    self.C = C\n","\n","  def __len__(self):\n","    return len(self.annotations)\n","\n","  def __getitem__(self, index):\n","    label_path = os.path.join(self.label_dir, self.annotations.iloc[index,1])\n","    boxes = []\n","\n","    with open(label_path) as f:\n","      for label in f.readlines():\n","        # List comprehension that converts each component of the line from string format to either float or integer\n","        class_label, x, y, width, height = [\n","            float(x) if float(x) != int(float(x)) else int(x)\n","            for x in label.replace(\"\\n\",\"\").split()\n","        ]\n","        # Append bboxes for that particular label\n","        boxes.append([class_label, x, y, width, height])\n","\n","    img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n","    image = Image.open(img_path)\n","    boxes = torch.tensor(boxes)\n","\n","    if self.transform:\n","      image = self.transform(image)\n","\n","    label_matrix = torch.zeros((self.S, self.S, self.C + 5*self.B))\n","    for box in boxes:\n","      class_label, x, y, width, height = box.tolist()\n","      class_label = int(class_label)\n","\n","      # i=cell row and j=cell column --> get the cell in which midpoint lies\n","      i , j = int(self.S * y), int(self.S * x)\n","      # Then again scales down to 0-1\n","      x_cell, y_cell = self.S * x - j, self.S * y - i\n","      width_cell, height_cell = (\n","          width * self.S,\n","          height * self.S\n","      )\n","\n","      # Now fill in the label_matrix\n","      if label_matrix[i,j,20] == 0: # 20th index specifies if there is object or not\n","        label_matrix[i,j,20] = 1 # This means that cell has object\n","        box_coordinates = torch.tensor(\n","            [x_cell, y_cell, width_cell, height_cell]\n","        )\n","        label_matrix[i,j,21:25] = box_coordinates\n","        label_matrix[i,j,class_label] = 1 # Specifying that particular class is present\n","\n","    return image, label_matrix"]},{"cell_type":"markdown","source":["### model training"],"metadata":{"id":"uZumDTAPs18D"}},{"cell_type":"code","source":["import torch\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import torchvision.transforms.functional as FT\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","# from model import Yolov1\n","# from dataset import VOCDataset\n","# from utils import (\n","#     non_max_suppression,\n","#     mean_average_precision,\n","#     intersection_over_union,\n","#     cellboxes_to_boxes,\n","#     get_bboxes,\n","#     plot_image,\n","#     save_checkpoint,\n","#     load_checkpoint,\n","# )\n","# from loss import YoloLoss\n","\n","seed = 123\n","torch.manual_seed(seed)\n","\n","# Hyperparameters etc.\n","LEARNING_RATE = 0.001\n","DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n","BATCH_SIZE = 64\n","PIN_MEMORY = True\n","LOAD_MODEL = False\n","WEIGHT_DECAY = 0\n","EPOCHS = 5\n","MODEL_FILE = \"./content/drive/MyDrive/weights/yolov1_pascal.pt\"\n","IMG_DIR = \"/content/data/images\"\n","LABEL_DIR = \"/content/data/labels\""],"metadata":{"id":"J9qW-uU8s7ga"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose([transforms.Resize((448, 448)), transforms.ToTensor()])"],"metadata":{"id":"zcQuL5Zd0xVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = VOCDataset(\n","        \"/content/data/train.csv\",\n","        transform=transform,\n","        img_dir=IMG_DIR,\n","        label_dir=LABEL_DIR,\n","    )\n","\n","test_dataset = VOCDataset(\n","        \"/content/data/test.csv\", transform=transform, img_dir=IMG_DIR, label_dir=LABEL_DIR,\n","    )\n","\n","train_loader = DataLoader(\n","    dataset=train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","test_loader = DataLoader(\n","    dataset=test_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")"],"metadata":{"id":"kPrmPYgMs0wW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n","optimizer = optim.Adam(\n","    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",")\n","loss_fn = YoloLoss()\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n"],"metadata":{"id":"Z-gZ_LXHyBjp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Lw-38FfzLAn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687517164330,"user_tz":-345,"elapsed":3022,"user":{"displayName":"Anupam Wagle","userId":"02083722924920495805"}},"outputId":"08438a3f-6eba-413c-d5ab-d1a38e58f5bd"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  2.21it/s, loss=654]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 654.0750122070312\n","Train mAP: 0.0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  2.63it/s, loss=2.32e+4]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 23201.404296875\n","Train mAP: 0.2376224249601364\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  2.69it/s, loss=1.01e+4]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 10123.587890625\n","Train mAP: 0.010914698243141174\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  2.70it/s, loss=4.4e+4]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 43997.53125\n","Train mAP: 0.05589340254664421\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  2.70it/s, loss=5.56e+4]\n"]},{"output_type":"stream","name":"stdout","text":["Mean loss was 55561.8046875\n","Train mAP: 0.041912227869033813\n"]}],"source":["for epoch in range(EPOCHS):\n","    loop = tqdm(train_loader, leave=True)\n","    mean_loss = []\n","    model.train()\n","    # for x,y in train_loader:\n","    for batch_idx, (x, y) in enumerate(loop):\n","        x = x.to(DEVICE)\n","        y = y.to(DEVICE)\n","        out = model(x)\n","        loss = loss_fn(out, y)\n","        mean_loss.append(loss.item())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","        loop.set_postfix(loss = loss.item())\n","\n","\n","    print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")\n","\n","    pred_boxes, target_boxes = get_bboxes(\n","        train_loader, model, iou_threshold=0.5, threshold=0.4\n","    )\n","\n","    mean_avg_prec = mean_average_precision(\n","        pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n","    )\n","    print(f\"Train mAP: {mean_avg_prec}\")\n","    scheduler.step(mean_avg_prec)\n"]},{"cell_type":"code","source":["train_dataset = VOCDataset(\n","        \"/content/data/train.csv\",\n","        transform=transform,\n","        img_dir=IMG_DIR,\n","        label_dir=LABEL_DIR,\n","    )\n","\n","\n","train_loader = DataLoader(\n","    dataset=train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","model = YoloV1(split_size=7, num_boxes=2, num_classes=3).to(DEVICE)\n","model.load_state_dict(torch.load(MODEL_FILE, map_location=torch.device('cpu')))\n","model.to(device)\n","\n","\n","optimizer = optim.Adam(\n","    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",")\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n","loss_fn = YoloLoss()\n","\n","index = 0\n","for x, y in train_loader:\n","  x = x.to(DEVICE)\n","  for idx in range(1):\n","    bboxes = cellboxes_to_boxes(model(x))\n","    bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n","    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n","    index += 1\n","    if index == 5:\n","      break\n","  if index == 5:\n","    break"],"metadata":{"id":"PMQ6PGsdV9dQ"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}